#################### Start Project ####################

rm(list=ls())

#################### Get Libraries ####################

library(tidymodels)
library(tidyverse)
library(workflows)
library(tune)
library(corrplot)
library(clickR)

#################### Get the Dataset ####################

churndata<-read.csv("Telco_customer_churn.csv")

#################### Know the Dataset ####################

head(churndata)
names(churndata)
str(churndata)
descriptive(churndata)

#################### Data Cleaning and Manupulation ####################

######### Drop Null Values #########

churndata<- churndata %>%
            drop_na(Total.Charges)

######### Transform Variables #########

churndata$Churn.Value<-as.character(churndata$Churn.Value)

summary(churndata)

#################### Find Correlation among Numeric variables ####################

par(mfrow=c(2,5))
hist(churndata$Tenure.Months,main="",xlab="Tenure Months")
hist(churndata$Monthly.Charges,main="",xlab="Monthly Charges")
hist(churndata$Total.Charges,main="",xlab="Total Charges")
hist(churndata$Churn.Score,main="",xlab="Churn Score")
hist(churndata$CLTV,main="",xlab="CLTV")
boxplot(churndata$Tenure.Months, horizontal=TRUE)
boxplot(churndata$Monthly.Charges, horizontal=TRUE)
boxplot(churndata$Total.Charges, horizontal=TRUE)
boxplot(churndata$Churn.Score, horizontal=TRUE)
boxplot(churndata$CLTV,horizontal=TRUE)

cordata<- churndata %>%
            select(Tenure.Months,Monthly.Charges,Total.Charges,Churn.Score,CLTV)
cormat <-cor(cordata)
cormat
corrplot(cormat,method="circle",order="hclust",type="upper")

rm(list=c("cordata","cormat"))


#################### Machine learning model development ####################

#### **** this is an imbalanced two class classification project**** ####
######### Copy the Dataset for Model Development #########

churndata2 <- churndata %>%  
              subset(select= -c(1:9,ncol(churndata)))
descriptive(churndata2)

######### Implementation of Stratification  #########

churndata2 %>% 
  count(Churn.Label) %>% 
  mutate (prop = n/sum(n))

# Splitting the Data for Train and Test considering Stratification

data_split<- initial_split(churndata2, strata=Churn.Value) 

str(data_split) # Overview of the Split

churn_train <- training(data_split) # TRAIN SET
churn_test  <- testing(data_split)  # TEST SET

# Check the Percentage after Stratification in Train and Test Data

nrow(churn_train)/nrow(churndata2)
churn_train %>% 
  count(Churn.Value) %>% 
  mutate(prop = n/sum(n))
churn_test %>% 
  count(Churn.Value) %>% 
  mutate(prop = n/sum(n))

# Cross Validation Calculation

churn_cv <-  vfold_cv(churn_train, strata = Churn.Value)

######## Start Model Developing ########

######## Create the RECIPE including the steps ########

churn_recipe<- churndata2 %>%
  recipe (Churn.Value ~ .)%>%
  step_corr(all_numeric_predictors())%>%
  step_center(all_numeric_predictors())

######## Preprocess the Train Model ########

churn_train_preprocess<-churn_recipe %>%
                        prep(churn_train) %>%
                        juice()
churn_train_preprocess



############### Specify the Different Models ###############


#### LOGISTIC REGRESSION MODEL ####

logistic_spec <-                  # model specification
  logistic_reg() %>%              # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification")      # model mode

logistic_spec                     # Show model specification

#### RANDOM FOREST MODEL ####

library(ranger)

rndmfrst_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

rndmfrst_spec

#### BOOSTED TREE(XGBOOST) MODEL ####

library(xgboost)
xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 
xgb_spec

#### K NEAREST NEIGHBOUR MODEL ####

knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
knn_spec

#### NEURAL NETWORK MODEL #####

library(keras)
nnet_spec<-
  mlp() %>%
  set_mode("classification") %>% 
  set_engine("keras", verbose = 0) 
nnet_spec



############### Create Workflow ###############




#### Logistic Regression Workflow ####

logistic_wflow <-               # Logistic workflow object
  workflow() %>%                # use workflow function
  add_recipe(churn_recipe) %>%  # use the new recipe
  add_model(logistic_spec)      # add logistic model spec

# show object
logistic_wflow

######## Random Forest Workflow  #########

rndmfrst_wflow <-                # Random Forest workflow object
  workflow() %>%                # use workflow function
  add_recipe(churn_recipe) %>%  # use the new recipe
  add_model(rndmfrst_spec)      # add Random Forest model spec
rndmfrst_wflow

######## Boosted Tree (XGBOOST) Workflow #########

xgb_wflow <-                     # Boosted Tree workflow object
  workflow() %>%                # use workflow function
  add_recipe(churn_recipe) %>%  # use the new recipe
  add_model(xgb_spec)           # add Boosted Tree model spec
xgb_wflow

######## K NEAREST NEIGHBOUR Workflow  #########

knn_wflow <-                     # Knn workflow object
  workflow() %>%                # use workflow function
  add_recipe(churn_recipe) %>%  # use the new recipe
  add_model(knn_spec)           # add Knn model spec
knn_wflow

######## Neural Network Workflow  #########

nnet_wflow <-                   # Neural Network workflow object
  workflow() %>%                # use workflow function
  add_recipe(churn_recipe) %>%  # use the new recipe
  add_model(nnet_spec)          # add Knn model spec
nnet_wflow



############### Evaluation of the Models ###############

# Write a Function to extract the results from the performance metrics

######## Logistic model Evaluation #########

get_model <- function(x) {
                          pull_workflow_fit(x) %>% 
                          tidy()
                         }

logistic_result <- 
  logistic_wflow %>% 
  fit_resamples(
                resamples = churn_cv,       # Cross Validation
                metrics = metric_set(recall, precision, f_meas,accuracy,kap,roc_auc,yardstick::sens,yardstick::spec ),
                control = control_resamples(
                save_pred = TRUE,
                extract = get_model)
                ) 
logistic_result %>%  collect_metrics(summarize = TRUE)
logistic_result %>%  collect_metrics(summarize = FALSE)

logistic_result$.extracts[[1]][[1]]
all_coef <- map_dfr(logistic_result$.extracts, ~ .x[[1]][[1]])
filter(all_coef, term == "Monthly.Charges")

logistic_pred <- logistic_result %>%
                 collect_predictions()

logistic_pred %>% 
  conf_mat(Churn.Value, .pred_class)%>% 
  autoplot(type = "heatmap")

logistic_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(Churn.Value,.pred_class) %>% 
  autoplot()

############ To be Continued ##########

# log_res_2 <- 
#   log_wflow %>% 
#   fit_resamples(
#                 resamples = cv_folds, 
#                 metrics = metric_set(
#                                      recall, precision, f_meas,accuracy,kap,roc_auc, sens, spec),
#                 control = control_resamples(
#                 save_pred = TRUE,
#                 extract = get_model) # use extract and our new function
#   ) 
